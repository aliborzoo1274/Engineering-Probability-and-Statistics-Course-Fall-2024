{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc41bdb-74f8-498b-8fbd-a8acb4b7ff93",
   "metadata": {},
   "source": [
    "# Third Question: Spam Email\n",
    "## Ali Borzoozadeh: 810102410"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87de83e2-1ea3-4ff8-b98d-7840faf7fb86",
   "metadata": {},
   "source": [
    "### A. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43dfe48-2731-4dc9-ab90-c87367921ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "emails = pd.read_csv('emails.csv')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Join tokens back to a single string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "emails['text'] = emails['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed521826-b042-414d-b86a-2b81ce4c64ed",
   "metadata": {},
   "source": [
    "### B. Dividing into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0aee05f-dc44-41e6-9038-28c5d9fcbb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feature\n",
    "X = emails['text']\n",
    "# label\n",
    "y = emails['spam']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bfdc7f-6940-4a1e-8e1a-712c5ebf7fb3",
   "metadata": {},
   "source": [
    "### C. Building BoW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a483ac-73ed-4c5b-8b46-7f8207d424cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform it into a BoW matrix\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Get the feature names (unique words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert BoW matrix to DataFrame for easier manipulation\n",
    "bow_df = pd.DataFrame(X_train_bow.toarray(), columns=feature_names)\n",
    "\n",
    "# Add the labels to the DataFrame\n",
    "bow_df['spam'] = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e86a24-08e0-46f0-9aea-24866139240f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=spam): 0.23526844172850284\n",
      "P(y=non-spam): 0.7647315582714972\n",
      "P(x_i | y=spam): aa          0.000004\n",
      "aaa         0.000004\n",
      "aadedeji    0.000004\n",
      "aagrawal    0.000004\n",
      "aal         0.000008\n",
      "              ...   \n",
      "zygoma      0.000008\n",
      "zymg        0.000015\n",
      "zzn         0.000008\n",
      "zzncacst    0.000004\n",
      "zzzz        0.000027\n",
      "Length: 30491, dtype: float64\n",
      "P(x_i | y=non-spam): aa          0.000090\n",
      "aaa         0.000019\n",
      "aadedeji    0.000002\n",
      "aagrawal    0.000002\n",
      "aal         0.000001\n",
      "              ...   \n",
      "zygoma      0.000001\n",
      "zymg        0.000001\n",
      "zzn         0.000001\n",
      "zzncacst    0.000004\n",
      "zzzz        0.000001\n",
      "Length: 30491, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Calculate P(y)\n",
    "P_spam = y_train.mean()\n",
    "P_non_spam = 1 - P_spam\n",
    "\n",
    "# Calculate P(x_i | y)\n",
    "spam_bow = bow_df[bow_df['spam'] == 1].drop(columns='spam')\n",
    "non_spam_bow = bow_df[bow_df['spam'] == 0].drop(columns='spam')\n",
    "\n",
    "word_counts_spam = spam_bow.sum(axis=0) + 1  # Add-one smoothing\n",
    "word_counts_non_spam = non_spam_bow.sum(axis=0) + 1  # Add-one smoothing\n",
    "\n",
    "total_spam_words = word_counts_spam.sum() + len(feature_names)  # Add-one smoothing\n",
    "total_non_spam_words = word_counts_non_spam.sum() + len(feature_names)  # Add-one smoothing\n",
    "\n",
    "P_xi_given_spam = word_counts_spam / total_spam_words\n",
    "P_xi_given_non_spam = word_counts_non_spam / total_non_spam_words\n",
    "\n",
    "print(f\"P(y=spam): {P_spam}\")\n",
    "print(f\"P(y=non-spam): {P_non_spam}\")\n",
    "print(f\"P(x_i | y=spam): {P_xi_given_spam}\")\n",
    "print(f\"P(x_i | y=non-spam): {P_xi_given_non_spam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a42605-c5aa-43a1-ae80-c8162e9952ba",
   "metadata": {},
   "source": [
    "### D. Prediction using Bayes' rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "911c9c20-afd5-4eeb-ad93-b3987d860441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.87%\n"
     ]
    }
   ],
   "source": [
    "# Convert test data to BoW matrix\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Ensure arrays have the same length as feature names\n",
    "P_xi_given_spam = P_xi_given_spam.reindex(feature_names, fill_value=1/total_spam_words)\n",
    "P_xi_given_non_spam = P_xi_given_non_spam.reindex(feature_names, fill_value=1/total_non_spam_words)\n",
    "\n",
    "# Predict the class for each email in the test set\n",
    "predictions = []\n",
    "for i in range(X_test_bow.shape[0]):\n",
    "    email_vector = X_test_bow[i].toarray()[0]\n",
    "    P_spam_email = P_spam * np.prod(np.power(P_xi_given_spam, email_vector))\n",
    "    P_non_spam_email = P_non_spam * np.prod(np.power(P_xi_given_non_spam, email_vector))\n",
    "    if P_spam_email > P_non_spam_email:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(np.array(predictions) == y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f8ffb-4921-4797-a545-1dbcc47e5a4c",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Laplace Smoothing, also known as add-one smoothing, is a technique used to handle the problem of zero probabilities. It works by adding one to the count of each word in the vocabulary, ensuring that no word has a zero probability.\n",
    "In this project we added + 1 to the word counts for both spam and non-spam emails to implement Laplace Smoothing (Part C)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad2e00-58c6-4f90-8d60-e9fdf1603af1",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "1. Log probabilities: Using logarithms to compute probabilities helps avoid numerical underflow.\n",
    "2. Log likelihood: For each email in the test set, calculate the log likelihoods for both spam and non-spam classes.\n",
    "\n",
    "Now we can calculate the accuracy and compare it to the former approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e10c644-044e-4d93-a66d-4b14ea05a471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.08%\n"
     ]
    }
   ],
   "source": [
    "# Convert test data to BoW matrix\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Ensure log probabilities arrays have the same length as feature names\n",
    "log_P_xi_given_spam = np.log(P_xi_given_spam.reindex(feature_names, fill_value=1/total_spam_words))\n",
    "log_P_xi_given_non_spam = np.log(P_xi_given_non_spam.reindex(feature_names, fill_value=1/total_non_spam_words))\n",
    "\n",
    "# Predict the class for each email in the test set\n",
    "log_P_spam = np.log(P_spam)\n",
    "log_P_non_spam = np.log(P_non_spam)\n",
    "\n",
    "predictions = []\n",
    "for i in range(X_test_bow.shape[0]):\n",
    "    email_vector = X_test_bow[i].toarray()[0]\n",
    "    log_likelihood_spam = log_P_spam + (email_vector * log_P_xi_given_spam).sum()\n",
    "    log_likelihood_non_spam = log_P_non_spam + (email_vector * log_P_xi_given_non_spam).sum()\n",
    "    if log_likelihood_spam > log_likelihood_non_spam:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(np.array(predictions) == y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b2cb9-facc-449b-858b-5ff3ec601f3e",
   "metadata": {},
   "source": [
    "#### As we can see, we reached to 98.08% from 87.87%. This approach should be much faster and provide an accurate estimate of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa290664-1f07-40eb-9b51-481b3f0481b9",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "To implement this, we need to preprocess the text to remove stop words from the first and rebuild the BoWâ€Œ model without stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112e10a1-5fef-41ac-91ad-86c3be13d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "emails = pd.read_csv('emails.csv')\n",
    "\n",
    "# Define stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "emails['text'] = emails['text'].apply(preprocess_text)\n",
    "\n",
    "# feature\n",
    "X = emails['text']\n",
    "# label\n",
    "y = emails['spam']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform it into a BoW matrix\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Get the feature names (unique words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert BoW matrix to DataFrame for easier manipulation\n",
    "bow_df = pd.DataFrame(X_train_bow.toarray(), columns=feature_names)\n",
    "\n",
    "# Add the labels to the DataFrame\n",
    "bow_df['spam'] = y_train.values\n",
    "\n",
    "# Calculate P(y)\n",
    "P_spam = y_train.mean()\n",
    "P_non_spam = 1 - P_spam\n",
    "\n",
    "# Calculate P(x_i | y)\n",
    "spam_bow = bow_df[bow_df['spam'] == 1].drop(columns='spam')\n",
    "non_spam_bow = bow_df[bow_df['spam'] == 0].drop(columns='spam')\n",
    "\n",
    "# Add-one smoothing\n",
    "word_counts_spam = spam_bow.sum(axis=0) + 1  # Add-one smoothing\n",
    "word_counts_non_spam = non_spam_bow.sum(axis=0) + 1  # Add-one smoothing\n",
    "\n",
    "# Add-one smoothing to total words\n",
    "total_spam_words = word_counts_spam.sum() + len(feature_names)  # Add-one smoothing\n",
    "total_non_spam_words = word_counts_non_spam.sum() + len(feature_names)  # Add-one smoothing\n",
    "\n",
    "P_xi_given_spam = word_counts_spam / total_spam_words\n",
    "P_xi_given_non_spam = word_counts_non_spam / total_non_spam_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "341717ff-7004-46bb-ada5-745b49b8349a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.27%\n"
     ]
    }
   ],
   "source": [
    "# Convert test data to BoW matrix\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Ensure arrays have the same length as feature names\n",
    "P_xi_given_spam = P_xi_given_spam.reindex(feature_names, fill_value=1/total_spam_words)\n",
    "P_xi_given_non_spam = P_xi_given_non_spam.reindex(feature_names, fill_value=1/total_non_spam_words)\n",
    "\n",
    "# Predict the class for each email in the test set\n",
    "predictions = []\n",
    "for i in range(X_test_bow.shape[0]):\n",
    "    email_vector = X_test_bow[i].toarray()[0]\n",
    "    P_spam_email = P_spam * np.prod(np.power(P_xi_given_spam, email_vector))\n",
    "    P_non_spam_email = P_non_spam * np.prod(np.power(P_xi_given_non_spam, email_vector))\n",
    "    if P_spam_email > P_non_spam_email:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(np.array(predictions) == y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af348d-a0d8-4470-a3d1-82fdec06d071",
   "metadata": {},
   "source": [
    "#### In this implementation we didn't use logarithms. Now let's use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f12db987-8f4b-46ce-af2a-ca4394827f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.34%\n"
     ]
    }
   ],
   "source": [
    "# Convert test data to BoW matrix\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Ensure log probabilities arrays have the same length as feature names\n",
    "log_P_xi_given_spam = np.log(P_xi_given_spam.reindex(feature_names, fill_value=1/total_spam_words))\n",
    "log_P_xi_given_non_spam = np.log(P_xi_given_non_spam.reindex(feature_names, fill_value=1/total_non_spam_words))\n",
    "\n",
    "# Predict the class for each email in the test set\n",
    "log_P_spam = np.log(P_spam)\n",
    "log_P_non_spam = np.log(P_non_spam)\n",
    "\n",
    "predictions = []\n",
    "for i in range(X_test_bow.shape[0]):\n",
    "    email_vector = X_test_bow[i].toarray()[0]\n",
    "    log_likelihood_spam = log_P_spam + (email_vector * log_P_xi_given_spam).sum()\n",
    "    log_likelihood_non_spam = log_P_non_spam + (email_vector * log_P_xi_given_non_spam).sum()\n",
    "    if log_likelihood_spam > log_likelihood_non_spam:\n",
    "        predictions.append(1)\n",
    "    else:\n",
    "        predictions.append(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(np.array(predictions) == y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c426814-5a02-4775-b81e-34f5b2f31390",
   "metadata": {},
   "source": [
    "#### Now we see that the accuracy is higher than ever by using logarithms and removing stop words!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
